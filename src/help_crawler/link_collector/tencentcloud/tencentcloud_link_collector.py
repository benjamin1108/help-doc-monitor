import asyncio
import json
import time
import yaml
import glob
from pathlib import Path
from urllib.parse import urljoin
from datetime import datetime, timedelta

from playwright.async_api import async_playwright


class TencentCloudLinkCollector:
    """è…¾è®¯äº‘å¸®åŠ©æ–‡æ¡£çˆ¬è™«
    
    é’ˆå¯¹è…¾è®¯äº‘æ–‡æ¡£ä¾§è¾¹æ  DOM ç»“æ„è¿›è¡Œé€‚é…ï¼Œæ”¯æŒæ·±å±‚çº§èœå•å±•å¼€ã€‚
    """

    def __init__(self, config=None, config_file: str = "config.yaml") -> None:
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼‰
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆconfigä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
        """
        if config is not None:
            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é…ç½®å­—å…¸
            tc_conf = config
        else:
            # ä»æ–‡ä»¶åŠ è½½é…ç½®
            self.raw_config = self._load_config(config_file)
            tc_conf = self.raw_config.get("tencentcloud", {})

            if not tc_conf:
                raise ValueError("config.yaml ç¼ºå°‘ tencentcloud èŠ‚ç‚¹")

        # åŸºæœ¬é…ç½®
        self.base_url: str = tc_conf.get("base_url", "https://cloud.tencent.com")
        self.crawler_settings: dict = tc_conf.get("crawler_settings", {})
        self.output_settings: dict = tc_conf.get("output_settings", {})
        self.products: dict = tc_conf.get("products", {})
        self.clicked_elements = set()

        # åˆå§‹åŒ–å†…å®¹æå–å™¨
        extractor_config = tc_conf.get('content_extractor', {'type': 'simple'})
        self.content_extractor = get_content_extractor(extractor_config)
        
        # ç¡®ä¿é“¾æ¥è¾“å‡ºç›®å½•å­˜åœ¨
        base_output_dir = Path(self.output_settings.get("base_dir", "out"))
        self.output_dir = base_output_dir / "links" / "tencentcloud"
        self.output_dir.mkdir(parents=True, exist_ok=True)

    @staticmethod
    def _load_config(config_file: str):
        with open(config_file, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)

    async def _wait_dom(self, page, ms: int | None = None):
        await page.wait_for_load_state("domcontentloaded", timeout=self.crawler_settings.get("wait_timeout", 10000))
        if ms is None:
            ms = int(self.crawler_settings.get("click_delay", 0.2) * 1000)
        await asyncio.sleep(ms / 1000)

    async def _expand_all_menus_dfs(self, page):
        """
        ä½¿ç”¨æ·±åº¦ä¼˜å…ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ç‚¹å‡»å±•å¼€æ‰€æœ‰å¯æŠ˜å çš„ä¾§è¾¹æ èœå•ã€‚
        """
        debug = self.crawler_settings.get("debug_mode", False)
        if debug:
            print("ğŸ” [DFS] å¼€å§‹å±•å¼€æ‰€æœ‰èœå•...")

        processed_nodes = set()
        
        while True:
            # åœ¨æ¯æ¬¡å¾ªç¯è¿­ä»£æ—¶é‡æ–°è·å– sidebar å…ƒç´ ï¼Œä»¥é¿å…å…ƒç´ è¿‡æ—¶ (stale element)
            sidebar = await page.query_selector(".doc-aside-wrap")
            if not sidebar:
                print("âš ï¸ [DFS] æœªæ‰¾åˆ°æˆ–ä¾§è¾¹æ å·²æ¶ˆå¤±ã€‚")
                break

            # æŸ¥æ‰¾æ‰€æœ‰å½“å‰å¯è§çš„å¯å±•å¼€é¡¹çš„ **ç‚¹å‡»ç›®æ ‡**ï¼ˆ<a> æ ‡ç­¾ï¼‰
            # è¿™äº›æ˜¯å°šæœªå±•å¼€çš„ J-expandable å…ƒç´ çš„ç›´æ¥å­ a.J-navLayer
            expandable_links_selector = ".J-expandable:not(.active) > a.J-navLayer"
            
            clickable_links = await sidebar.query_selector_all(expandable_links_selector)
            
            # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„èŠ‚ç‚¹ï¼Œé˜²æ­¢æ­»å¾ªç¯
            links_to_click = []
            for link in clickable_links:
                node_id = await link.get_attribute("data-node")
                # åªå¤„ç†å¯è§çš„ã€æœªå¤„ç†è¿‡çš„èŠ‚ç‚¹
                is_visible = await link.is_visible()
                if is_visible and node_id and node_id not in processed_nodes:
                    links_to_click.append(link)
                    processed_nodes.add(node_id)
            
            if not links_to_click:
                # å¦‚æœæ²¡æœ‰æ›´å¤šå¯å±•å¼€çš„é“¾æ¥ï¼Œè¯´æ˜å·²ç»å…¨éƒ¨å±•å¼€
                if debug:
                    print("âœ… [DFS] æ²¡æœ‰æ›´å¤šå¯å±•å¼€çš„èœå•ï¼Œå±•å¼€å®Œæˆã€‚")
                break

            if debug:
                print(f"  â–¶ï¸ [DFS] å‘ç° {len(links_to_click)} ä¸ªæ–°çš„å¯å±•å¼€èœå•ï¼Œæ­£åœ¨å¤„ç†...")

            # ä¾æ¬¡ç‚¹å‡»æ‰¾åˆ°çš„é“¾æ¥ä»¥å±•å¼€å­èœå•
            for i, link_to_click in enumerate(links_to_click):
                try:
                    text = await link_to_click.text_content() or "æœªçŸ¥èœå•"
                    await link_to_click.click(timeout=5000)
                    if debug and i % 10 == 0:
                        print(f"    ğŸ–±ï¸ [DFS] å·²ç‚¹å‡»: {text.strip()}")
                    # ç­‰å¾…ä¸€ä¸‹ï¼Œè®© JS æœ‰æ—¶é—´æ¸²æŸ“ DOM
                    await self._wait_dom(page, 50) 
                except Exception as e:
                    if debug:
                        text_content = await link_to_click.text_content()
                        print(f"    âŒ [DFS] ç‚¹å‡» '{text_content.strip()}' å¤±è´¥: {e}")
            
            # çŸ­æš‚ç­‰å¾…ï¼Œç¡®ä¿æ‰€æœ‰ç‚¹å‡»æ“ä½œçš„DOMæ›´æ–°éƒ½å·²å®Œæˆ
            await self._wait_dom(page, self.crawler_settings.get("click_delay", 0.2) * 1000)

    async def _collect_all_links_from_sidebar(self, page):
        """
        åœ¨æ‰€æœ‰èœå•éƒ½å±•å¼€åï¼Œä»ä¾§è¾¹æ æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æ–‡æ¡£é“¾æ¥ã€‚
        """
        debug = self.crawler_settings.get("debug_mode", False)
        if debug:
            print("ğŸ”— [Collect] å¼€å§‹æ”¶é›†æ‰€æœ‰é“¾æ¥...")

        sidebar = await page.query_selector(".doc-aside-wrap")
        if not sidebar:
            return []

        # è·å–æ‰€æœ‰å¯¼èˆªé“¾æ¥
        all_link_elements = await sidebar.query_selector_all("a.J-navLayer")
        if debug:
            print(f"  ğŸ” [Collect] æ‰¾åˆ° {len(all_link_elements)} ä¸ª a.J-navLayer å…ƒç´ ã€‚")

        results = []
        seen_urls = set()

        for link in all_link_elements:
            href = await link.get_attribute("href") or ""
            text = (await link.text_content() or "").strip()

            # å¿½ç•¥æ— æ•ˆæ¡ç›®
            if not text or not href or href.startswith("javascript:"):
                continue

            # æ„å»ºç»å¯¹URL
            final_url = urljoin(self.base_url, href)
            
            # è¿‡æ»¤éè…¾è®¯äº‘æ–‡æ¡£é“¾æ¥
            if not final_url.startswith(self.base_url):
                 continue

            # å»é‡
            if final_url in seen_urls:
                continue
            seen_urls.add(final_url)

            results.append({"url": final_url, "title": text})

        if debug:
            print(f"âœ… [Collect] æ”¶é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆæ–‡æ¡£é“¾æ¥ã€‚")
        return results

    async def _crawl_single_doc(self, page, url: str, title: str):
        """å¦‚éœ€æŠ“å–æ­£æ–‡å†…å®¹ï¼Œå¯åœ¨ config.yaml å°† include_content è®¾ä¸º true"""
        if not self.output_settings.get("include_content", False):
            return {"url": url, "title": title, "crawl_time": datetime.now().isoformat()}

        try:
            await page.goto(url, timeout=self.crawler_settings.get("wait_timeout", 20000), wait_until="domcontentloaded")
            await asyncio.sleep(0.3)

            content = ""
            selectors = [".article-wrap", ".markdown-body", "main", ".article-content"]
            for sel in selectors:
                node = await page.query_selector(sel)
                if node:
                    txt = await node.text_content()
                    if txt and len(txt) > 50:
                        content = txt.strip()
                        break

            return {"url": url, "title": title, "content": content, "crawl_time": datetime.now().isoformat()}
        except Exception as e:
            return {"url": url, "title": title, "content": "", "error": str(e), "crawl_time": datetime.now().isoformat()}

    async def _save_product(self, key: str, info: dict, docs: list[dict]):
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        links_file = self.output_dir / f"tencentcloud_{key}_links_{ts}.txt"

        with open(links_file, "w", encoding="utf-8") as f:
            f.write(f"{info['name']} å¸®åŠ©æ–‡æ¡£é“¾æ¥\n")
            f.write("=" * 50 + "\n")
            f.write(f"äº§å“: {info['name']}\n")
            f.write(f"æè¿°: {info['description']}\n")
            f.write(f"èµ·å§‹URL: {info['url']}\n")
            f.write(f"æ–‡æ¡£æ•°é‡: {len(docs)}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {ts}\n")
            f.write("=" * 50 + "\n\n")
            for idx, doc in enumerate(docs, 1):
                f.write(f"{idx:3d}. {doc['title']}\n")
                f.write(f"     {doc['url']}\n\n")

        if self.output_settings.get("include_content", False):
            json_output_dir = self.output_dir.parent / "content" / "json" / "tencentcloud"
            json_output_dir.mkdir(parents=True, exist_ok=True)
            
            json_file = json_output_dir / f"tencentcloud_{key}_data_{ts}.json"
            with open(json_file, "w", encoding="utf-8") as jf:
                json.dump({"product": info, "docs": docs, "timestamp": ts}, jf, ensure_ascii=False, indent=2)

        return links_file

    def _should_skip_crawl(self, key: str) -> bool:
        """
        æ ¹æ®æ–‡ä»¶æ—¶é—´æˆ³å’Œé…ç½®çš„é—´éš”ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡çˆ¬å–ã€‚
        """
        interval_hours = self.output_settings.get("recrawl_interval_hours")
        if not interval_hours or not isinstance(interval_hours, (int, float)) or interval_hours <= 0:
            return False

        # æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        search_pattern = str(self.output_dir / f"tencentcloud_{key}_links_*.txt")
        existing_files = glob.glob(search_pattern)
        if not existing_files:
            return False

        latest_file = max(existing_files, key=lambda p: Path(p).stat().st_mtime)
        file_mod_time = datetime.fromtimestamp(Path(latest_file).stat().st_mtime)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ‰æ•ˆæœŸé—´å†…
        if datetime.now() - file_mod_time < timedelta(hours=interval_hours):
            print(f"âœ… äº§å“ '{key}' åœ¨ {interval_hours} å°æ—¶å†…å·²æœ‰æ–°æ–‡ä»¶ï¼Œæœ¬æ¬¡è·³è¿‡çˆ¬å–ã€‚")
            print(f"   ğŸ“„ æ–‡ä»¶: {Path(latest_file).name}")
            return True
            
        return False

    async def crawl_product(self, key: str, info: dict):
        if self._should_skip_crawl(key):
            return

        print(f"\nğŸš€ å¼€å§‹çˆ¬å–: {info['name']}")
        print(f"ğŸ“ URL: {info['url']}")
        print("-" * 60)

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.crawler_settings.get("headless", True),
                                              args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-images"])
            try:
                context = await browser.new_context()
                page = await context.new_page()

                t0 = time.time()
                # 1. æ‰“å¼€é¡µé¢
                print("1ï¸âƒ£  åŠ è½½é¡µé¢...")
                await page.goto(info['url'], timeout=self.crawler_settings.get("wait_timeout", 20000), wait_until="domcontentloaded")
                await self._wait_dom(page, 500)
                print(f"âœ“ é¡µé¢åŠ è½½å®Œæˆ ({time.time() - t0:.1f}s)")

                # 2. ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•ï¼ˆå¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼ï¼‰
                if self.crawler_settings.get("debug_mode", False):
                    print("ğŸ” ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•...")
                    html_content = await page.content()
                    debug_file = self.output_dir / f"debug_{key}_page.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    print(f"ğŸ“„ é¡µé¢HTMLå·²ä¿å­˜: {debug_file.name}")

                # 3. å±•å¼€ä¾§è¾¹æ  (NEW LOGIC)
                print("2ï¸âƒ£  æ·±åº¦å±•å¼€èœå• (DFS)...")
                t1 = time.time()
                await self._expand_all_menus_dfs(page)
                print(f"âœ“ èœå•å±•å¼€å®Œæˆ ({time.time() - t1:.1f}s)")

                # 4. æ”¶é›†é“¾æ¥ (NEW LOGIC)
                print("3ï¸âƒ£  æ”¶é›†æ–‡æ¡£é“¾æ¥...")
                docs_info = await self._collect_all_links_from_sidebar(page)
                print(f"âœ“ å…±æ”¶é›†åˆ° {len(docs_info)} æ¡è®°å½•")
                if not docs_info:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£é“¾æ¥ï¼Œè·³è¿‡è¯¥äº§å“")
                    return None

                # 5. å¯é€‰æŠ“å–æ­£æ–‡
                final_docs = []
                if self.output_settings.get("include_content", False):
                    print("4ï¸âƒ£  æŠ“å–æ­£æ–‡å†…å®¹ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
                    for idx, doc_info in enumerate(docs_info, 1):
                        if idx % 20 == 0:
                            print(f"   è¿›åº¦: {idx}/{len(docs_info)}")
                        # Pass url and title from the dict
                        doc_content = await self._crawl_single_doc(page, doc_info['url'], doc_info['title'])
                        final_docs.append(doc_content)
                        if idx < len(docs_info):
                            await asyncio.sleep(self.crawler_settings.get("crawl_delay", 0.5))
                else:
                    # The data is already in the right format, just need to add crawl_time
                    for doc_info in docs_info:
                        doc_info['crawl_time'] = datetime.now().isoformat()
                    final_docs = docs_info

                # 6. ä¿å­˜
                print("5ï¸âƒ£  ä¿å­˜ç»“æœ...")
                links_path = await self._save_product(key, info, final_docs)
                elapsed = time.time() - t0
                print(f"âœ… {info['name']} çˆ¬å–å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}sï¼Œé“¾æ¥æ–‡ä»¶: {links_path.name}")

                return {"product_key": key, "product_name": info['name'], "total_docs": len(final_docs),
                        "links_file": str(links_path), "duration": elapsed}
            finally:
                await browser.close()

    async def crawl_all_products(self, selected_products: list[str] | None = None):
        products = self.products if not selected_products else {k: v for k, v in self.products.items() if k in selected_products}
        print(f"ğŸ¯ è®¡åˆ’çˆ¬å– {len(products)} ä¸ªäº§å“æ–‡æ¡£")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir.resolve()}")
        print("=" * 70)

        results = []
        all_start = time.time()
        for idx, (k, info) in enumerate(products.items(), 1):
            print(f"\n[{idx}/{len(products)}] å½“å‰äº§å“: {info['name']} ({k})")
            try:
                res = await self.crawl_product(k, info)
                if res:
                    results.append(res)
            except Exception as exc:
                print(f"âŒ çˆ¬å– {info['name']} å¤±è´¥: {exc}")
                continue

        # æ±‡æ€»
        print(f"\nğŸ æ‰€æœ‰äº§å“çˆ¬å–å®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - all_start:.1f}s")
        return results


# å½“ç›´æ¥æ‰§è¡Œè¯¥æ¨¡å—æ—¶ï¼Œé»˜è®¤å¯åŠ¨å•äº§å“çˆ¬å– (è´Ÿè½½å‡è¡¡)
if __name__ == "__main__":
    async def _self_test():
        crawler = TencentCloudLinkCollector()
        await crawler.crawl_all_products(["clb"])

    asyncio.run(_self_test())
