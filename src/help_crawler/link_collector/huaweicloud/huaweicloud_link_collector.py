import asyncio
import json
import time
import yaml
import glob
from pathlib import Path
from urllib.parse import urljoin
from datetime import datetime, timedelta

from playwright.async_api import async_playwright


class HuaweiCloudLinkCollector:
    """åä¸ºäº‘å¸®åŠ©æ–‡æ¡£çˆ¬è™«
    
    é’ˆå¯¹åä¸ºäº‘æ–‡æ¡£ä¾§è¾¹æ  DOM ç»“æ„è¿›è¡Œé€‚é…ï¼Œæ”¯æŒæ·±å±‚çº§èœå•å±•å¼€ã€‚
    """

    def __init__(self, config=None, config_file: str = "config.yaml") -> None:
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼‰
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆconfigä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
        """
        if config is not None:
            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„é…ç½®å­—å…¸
            hw_conf = config
        else:
            # ä»æ–‡ä»¶åŠ è½½é…ç½®
            self.raw_config = self._load_config(config_file)
            hw_conf = self.raw_config.get("huaweicloud", {})

            if not hw_conf:
                raise ValueError("config.yaml ç¼ºå°‘ huaweicloud èŠ‚ç‚¹")

        # åŸºæœ¬é…ç½®
        self.base_url: str = hw_conf.get("base_url", "https://support.huaweicloud.com")
        self.crawler_settings: dict = hw_conf.get("crawler_settings", {})
        self.output_settings: dict = hw_conf.get("output_settings", {})
        self.products: dict = hw_conf.get("products", {})
        self.clicked_elements = set()

        # è¾“å‡ºç›®å½•
        base_output_dir = Path(self.output_settings.get("base_dir", "out"))
        self.output_dir = base_output_dir / "links" / "huaweicloud"
        self.output_dir.mkdir(parents=True, exist_ok=True)

    @staticmethod
    def _load_config(config_file: str):
        with open(config_file, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)

    async def _wait_dom(self, page, ms: int | None = None):
        await page.wait_for_load_state("domcontentloaded", timeout=self.crawler_settings.get("wait_timeout", 10000))
        if ms is None:
            ms = int(self.crawler_settings.get("click_delay", 0.2) * 1000)
        await asyncio.sleep(ms / 1000)

    async def _collect_visible_links(self, page, results, seen_urls):
        """æ”¶é›†å½“å‰æ‰€æœ‰å¯è§çš„é“¾æ¥"""
        # æ¯æ¬¡éƒ½é‡æ–°æŸ¥è¯¢sidebarï¼Œé¿å…å…ƒç´ å¤±æ•ˆ
        sidebar = await page.query_selector("div.side-nav.sidenav-main")
        if not sidebar:
            return 0
            
        all_link_elements = await sidebar.query_selector_all("a.js-title.ajax-nav")
        new_links_count = 0
        base_url_for_join = self.base_url + "/" if not self.base_url.endswith("/") else self.base_url
        
        for link in all_link_elements:
            try:
                if not await link.is_visible():
                    continue

                href = await link.get_attribute("p-href") or await link.get_attribute("href") or ""
                text = (await link.text_content() or "").strip()

                if not text or not href or href.startswith("javascript:"):
                    continue

                # ä½¿ç”¨ urljoin ä¿è¯é“¾æ¥æ­£ç¡®æ‹¼æ¥
                final_url = urljoin(base_url_for_join, href)

                if final_url not in seen_urls:
                    seen_urls.add(final_url)
                    results.append({"url": final_url, "title": text})
                    new_links_count += 1
            except Exception:
                # å¿½ç•¥å•ä¸ªå…ƒç´ çš„é”™è¯¯ï¼Œç»§ç»­å¤„ç†å…¶ä»–å…ƒç´ 
                continue
                
        return new_links_count

    async def _expand_all_menus_dfs(self, page):
        """
        ä»¥æ·±åº¦ä¼˜å…ˆ(DFS)çš„è¿­ä»£æ–¹å¼ï¼Œæ¨¡æ‹Ÿç”¨æˆ·ç‚¹å‡»è¡Œä¸ºï¼Œå°†æ‰€æœ‰å¯å±•å¼€çš„èœå•é¡¹å…¨éƒ¨å±•å¼€ã€‚
        è¿™ä¸ªæ–¹æ³•åªè´Ÿè´£å±•å¼€ï¼Œä¸æ”¶é›†é“¾æ¥ï¼Œä»¥æé«˜æ•ˆç‡ã€‚
        """
        debug = self.crawler_settings.get("debug_mode", False)
        if debug:
            print("ğŸ” [Crawl] å¼€å§‹æ·±åº¦ä¼˜å…ˆå±•å¼€æ‰€æœ‰èœå•...")

        # å¾ªç¯ç›´åˆ°æ²¡æœ‰æ–°çš„å¯å±•å¼€é¡¹ä¸ºæ­¢
        while True:
            # æ¯æ¬¡å¾ªç¯éƒ½é‡æ–°æŸ¥è¯¢æ‰€æœ‰å…ƒç´ ï¼Œä¿è¯å¥å£®æ€§
            sidebar = await page.query_selector("div.side-nav.sidenav-main")
            if not sidebar:
                if debug:
                    print("âš ï¸ [Crawl] ä¾§è¾¹æ æ¶ˆå¤±ï¼Œç»“æŸæµç¨‹ã€‚")
                break

            expandable_selector = "li.nav-item:not(.unfold):has(> i.foldIcon) > a.js-title"
            
            link_to_click = None
            try:
                # æ‰¾åˆ°æ‰€æœ‰å¯å±•å¼€çš„é“¾æ¥
                potential_links = await sidebar.query_selector_all(expandable_selector)
                
                # æ·±åº¦ä¼˜å…ˆï¼šåªæ‰¾ç¬¬ä¸€ä¸ªå¯è§çš„è¿›è¡Œç‚¹å‡»
                for link in potential_links:
                    if await link.is_visible():
                        link_to_click = link
                        break
            except Exception as e:
                if debug:
                    print(f"    âŒ [Crawl] æŸ¥è¯¢å¯å±•å¼€èœå•æ—¶å‡ºé”™: {e}")
                break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯ç‚¹å‡»çš„é“¾æ¥ï¼Œè¯´æ˜å…¨éƒ¨å±•å¼€å®Œæ¯•
            if not link_to_click:
                if debug:
                    print("âœ… [Crawl] æ²¡æœ‰å‘ç°æ–°çš„å¯å±•å¼€èœå•ï¼Œå±•å¼€å®Œæˆã€‚")
                break
            
            try:
                if debug:
                    text = await link_to_click.text_content() or ""
                    print(f"  â–¶ï¸ [Crawl] ç‚¹å‡»å±•å¼€: {text.strip()}")
                
                await link_to_click.click(timeout=5000)
                await page.wait_for_timeout(50) # è½»é‡çº§ç­‰å¾…ï¼Œé¿å…è¿‡åº¦å»¶è¿Ÿ
            except Exception as e:
                if debug:
                    print(f"    âŒ [Crawl] ç‚¹å‡»èœå•å¤±è´¥: {e}ï¼Œå°è¯•è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯ã€‚")
                # å¦‚æœç‚¹å‡»å¤±è´¥ï¼ˆä¾‹å¦‚å…ƒç´ åœ¨æŸ¥è¯¢ååˆ°ç‚¹å‡»å‰æ¶ˆå¤±äº†ï¼‰ï¼Œå°±ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
                continue
        
        if debug:
            print("âœ… [Crawl] æ‰€æœ‰èœå•å±•å¼€å®Œæ¯•ã€‚")

    async def _collect_all_links_from_sidebar(self, page):
        """
        åœ¨æ‰€æœ‰èœå•éƒ½å±•å¼€åï¼Œä¸€æ¬¡æ€§æ”¶é›†ä¾§è¾¹æ ä¸­æ‰€æœ‰å¯è§çš„æ–‡æ¡£é“¾æ¥ã€‚
        """
        debug = self.crawler_settings.get("debug_mode", False)
        if debug:
            print("ğŸ” [Crawl] å¼€å§‹æ”¶é›†æ‰€æœ‰é“¾æ¥...")

        results = []
        seen_urls = set()
        
        # _collect_visible_links å†…éƒ¨ä¼šé‡æ–°æŸ¥è¯¢ sidebarï¼Œæ˜¯å®‰å…¨çš„
        await self._collect_visible_links(page, results, seen_urls)

        if debug:
            print(f"âœ… [Crawl] æ”¶é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆæ–‡æ¡£é“¾æ¥ã€‚")
        
        return results

    def _create_link_record(self, url: str, title: str):
        """åˆ›å»ºé“¾æ¥è®°å½•ï¼Œåªä¿å­˜é“¾æ¥ä¿¡æ¯"""
        return {"url": url, "title": title, "crawl_time": datetime.now().isoformat()}

    async def _save_product(self, key: str, info: dict, docs: list[dict]):
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        links_file = self.output_dir / f"huawei_{key}_links_{ts}.txt"

        with open(links_file, "w", encoding="utf-8") as f:
            f.write(f"{info['name']} å¸®åŠ©æ–‡æ¡£é“¾æ¥\n")
            f.write("=" * 50 + "\n")
            f.write(f"äº§å“: {info['name']}\n")
            f.write(f"æè¿°: {info['description']}\n")
            f.write(f"èµ·å§‹URL: {info['url']}\n")
            f.write(f"æ–‡æ¡£æ•°é‡: {len(docs)}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {ts}\n")
            f.write("=" * 50 + "\n\n")
            for idx, doc in enumerate(docs, 1):
                f.write(f"{idx:3d}. {doc['title']}\n")
                f.write(f"     {doc['url']}\n\n")

        return links_file

    def _should_skip_crawl(self, key: str) -> bool:
        """
        æ ¹æ®æ–‡ä»¶æ—¶é—´æˆ³å’Œé…ç½®çš„é—´éš”ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡çˆ¬å–ã€‚
        """
        recrawl_interval_hours = self.output_settings.get('recrawl_interval_hours', 24)

        # æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        search_pattern = str(self.output_dir / f"huawei_{key}_links_*.txt")
        existing_files = glob.glob(search_pattern)
        if not existing_files:
            return False

        latest_file = max(existing_files, key=lambda p: Path(p).stat().st_mtime)
        file_mod_time = datetime.fromtimestamp(Path(latest_file).stat().st_mtime)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ‰æ•ˆæœŸé—´å†…
        if datetime.now() - file_mod_time < timedelta(hours=recrawl_interval_hours):
            print(f"âœ… äº§å“ '{key}' åœ¨ {recrawl_interval_hours} å°æ—¶å†…å·²æœ‰æ–°æ–‡ä»¶ï¼Œæœ¬æ¬¡è·³è¿‡çˆ¬å–ã€‚")
            print(f"   ğŸ“„ æ–‡ä»¶: {Path(latest_file).name}")
            return True
            
        return False

    async def crawl_product(self, key: str, info: dict):
        if self._should_skip_crawl(key):
            return

        print(f"\nğŸš€ å¼€å§‹çˆ¬å–: {info['name']}")
        print(f"ğŸ“ URL: {info['url']}")
        print("-" * 60)

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.crawler_settings.get("headless", True),
                                              args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-images"])
            try:
                context = await browser.new_context()
                page = await context.new_page()

                t0 = time.time()
                print("1ï¸âƒ£  åŠ è½½é¡µé¢...")
                await page.goto(info['url'], timeout=self.crawler_settings.get("wait_timeout", 20000), wait_until="domcontentloaded")
                await page.wait_for_timeout(100)
                print(f"âœ“ é¡µé¢åŠ è½½å®Œæˆ ({time.time() - t0:.1f}s)")

                if self.crawler_settings.get("debug_mode", False):
                    print("ğŸ” ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•...")
                    html_content = await page.content()
                    debug_file = self.output_dir / f"debug_{key}_page.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    print(f"ğŸ“„ é¡µé¢HTMLå·²ä¿å­˜: {debug_file.name}")

                print("2ï¸âƒ£  åŠ¨æ€å±•å¼€æ‰€æœ‰èœå•...")
                t1 = time.time()
                await self._expand_all_menus_dfs(page)
                print(f"âœ“ èœå•å±•å¼€å®Œæˆ ({time.time() - t1:.1f}s)")
                
                print("3ï¸âƒ£  æ”¶é›†æ‰€æœ‰é“¾æ¥...")
                docs_info = await self._collect_all_links_from_sidebar(page)
                
                print(f"âœ“ å…±æ”¶é›†åˆ° {len(docs_info)} æ¡è®°å½•")
                if not docs_info:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£é“¾æ¥ï¼Œè·³è¿‡è¯¥äº§å“")
                    return None

                # åªä¿å­˜é“¾æ¥ä¿¡æ¯ï¼Œä¸æå–å†…å®¹
                final_docs = []
                for doc_info in docs_info:
                    final_docs.append(self._create_link_record(doc_info['url'], doc_info['title']))

                print("5ï¸âƒ£  ä¿å­˜ç»“æœ...")
                links_path = await self._save_product(key, info, final_docs)
                elapsed = time.time() - t0
                print(f"âœ… {info['name']} çˆ¬å–å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}sï¼Œé“¾æ¥æ–‡ä»¶: {links_path.name}")

                return {"product_key": key, "product_name": info['name'], "total_docs": len(final_docs),
                        "links_file": str(links_path), "duration": elapsed}
            finally:
                await browser.close()

    async def crawl_all_products(self, selected_products: list[str] | None = None):
        products = self.products if not selected_products else {k: v for k, v in self.products.items() if k in selected_products}
        print(f"ğŸ¯ è®¡åˆ’çˆ¬å– {len(products)} ä¸ªäº§å“æ–‡æ¡£")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir.resolve()}")
        print("=" * 70)

        results = []
        all_start = time.time()
        for idx, (k, info) in enumerate(products.items(), 1):
            print(f"\n[{idx}/{len(products)}] å½“å‰äº§å“: {info['name']} ({k})")
            try:
                res = await self.crawl_product(k, info)
                if res:
                    results.append(res)
            except Exception as exc:
                print(f"âŒ çˆ¬å– {info['name']} å¤±è´¥: {exc}")
                continue

        # æ±‡æ€»
        print(f"\nğŸ æ‰€æœ‰äº§å“çˆ¬å–å®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - all_start:.1f}s")
        return results


if __name__ == "__main__":
    async def _self_test():
        crawler = HuaweiCloudLinkCollector()
        # å‡è®¾åœ¨ config.yaml ä¸­å·²é…ç½®äº† 'vpc' äº§å“
        await crawler.crawl_all_products(["vpc"])

    asyncio.run(_self_test()) 