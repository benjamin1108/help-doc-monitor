import asyncio
import json
import time
import yaml
import os
from pathlib import Path
from playwright.async_api import async_playwright
from urllib.parse import urljoin
from datetime import datetime

class AliyunDocCrawler:
    def __init__(self, config=None, config_file="config.yaml"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨ï¼‰
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆconfigä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
        """
        if config is not None:
            self.config = config
        else:
            self.config = self.load_config(config_file)
            
        self.base_url = self.config['base_url']
        self.crawler_settings = self.config['crawler_settings']
        self.output_settings = self.config['output_settings']
        self.products = self.config['products']
        self.clicked_elements = set()
        
        # ç¡®ä¿é˜¿é‡Œäº‘ä¸“ç”¨è¾“å‡ºç›®å½•å­˜åœ¨
        base_output_dir = Path(self.output_settings['base_dir'])
        self.output_dir = base_output_dir / "aliyun"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def load_config(self, config_file):
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°")
            raise
        except yaml.YAMLError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    async def wait_for_update(self, page, ms=None):
        """ç­‰å¾…DOMæ›´æ–°"""
        if ms is None:
            ms = self.crawler_settings['click_delay'] * 1000
        await page.wait_for_load_state('domcontentloaded', timeout=self.crawler_settings['wait_timeout'])
        await asyncio.sleep(ms / 1000)
    
    async def _expand_all_menus_dfs(self, page):
        """
        ä½¿ç”¨è¿­ä»£ç‚¹å‡»çš„æ–¹å¼ï¼Œé«˜æ•ˆåœ°å±•å¼€æ‰€æœ‰å¯æŠ˜å çš„ä¾§è¾¹æ èœå•ã€‚
        è¯¥æ–¹æ³•å–ä»£äº†æ—§çš„ã€å¤æ‚çš„é€’å½’å±•å¼€é€»è¾‘ã€‚
        """
        debug = self.crawler_settings.get("debug_mode", False)
        if debug:
            print("ğŸ” [DFS-Expand] å¼€å§‹ä½¿ç”¨æ–°çš„DFSæ–¹æ³•å±•å¼€æ‰€æœ‰èœå•...")

        sidebar = await page.query_selector("#common-menu-container")
        if not sidebar:
            print("âš ï¸ [DFS-Expand] æœªæ‰¾åˆ° #common-menu-container ä¾§è¾¹æ å®¹å™¨ã€‚")
            return

        while True:
            # æ–°çš„é€‰æ‹©å™¨ç­–ç•¥:
            # æŸ¥æ‰¾æ‰€æœ‰è¡¨ç¤º"å…³é—­"çŠ¶æ€çš„å¯å±•å¼€èœå•çš„ç®­å¤´å›¾æ ‡ã€‚
            expandable_icon_selector = 'i.help-icon-close-arrow'
            
            icons_to_click = await sidebar.query_selector_all(expandable_icon_selector)
            
            visible_icons_to_click = []
            for icon in icons_to_click:
                if await icon.is_visible():
                    visible_icons_to_click.append(icon)

            if not visible_icons_to_click:
                if debug:
                    print("âœ… [DFS-Expand] æ²¡æœ‰æ›´å¤šå¯è§çš„ 'å…³é—­' çŠ¶æ€èœå•ï¼Œå±•å¼€å®Œæˆã€‚")
                break

            if debug:
                print(f"  â–¶ï¸ [DFS-Expand] å‘ç° {len(visible_icons_to_click)} ä¸ªæ–°çš„å¯å±•å¼€èœå•ï¼Œæ­£åœ¨å¤„ç†...")

            # ä¾æ¬¡ç‚¹å‡»å›¾æ ‡ä»¥å±•å¼€å­èœå•
            for icon in visible_icons_to_click:
                click_target = None # define here for except block
                try:
                    # å®šä½åˆ°çˆ¶çº§<a>æ ‡ç­¾ï¼Œè¿™æ˜¯æ›´å¯é çš„ç‚¹å‡»ç›®æ ‡
                    click_target = await icon.query_selector("xpath=./parent::a")
                    if not click_target:
                        if debug: print("    âš ï¸ [DFS-Expand] æœªæ‰¾åˆ°å›¾æ ‡çš„çˆ¶çº§<a>ï¼Œè·³è¿‡")
                        continue

                    # ç‚¹å‡»å‰ï¼Œç¡®ä¿å…ƒç´ åœ¨å¯è§†åŒºåŸŸå†…
                    await click_target.scroll_into_view_if_needed()
                    await page.wait_for_timeout(100) # ç­‰å¾…æ»šåŠ¨ç¨³å®š

                    text = await click_target.text_content() or "æœªçŸ¥èœå•"
                    await click_target.click(timeout=5000)
                    
                    if debug:
                        print(f"    ğŸ–±ï¸ [DFS-Expand] ç‚¹å‡»å±•å¼€: {text.strip()}")

                    # æ¯æ¬¡ç‚¹å‡»åç»™äºˆçŸ­æš‚å»¶æ—¶ï¼Œç­‰å¾…JSæ¸²æŸ“
                    await self.wait_for_update(page, 50)

                except Exception as e:
                    if debug:
                        failed_text = "æœªçŸ¥èœå•"
                        try:
                            if click_target:
                                failed_text = await click_target.text_content() or failed_text
                        except:
                            pass # è·å–æ–‡æœ¬å¤±è´¥å°±ç®—äº†
                        print(f"    âŒ [DFS-Expand] ç‚¹å‡» '{failed_text.strip()}' å¤±è´¥: {str(e)}")
            
            # ä¸€è½®ç‚¹å‡»å®Œæˆåï¼Œç­‰å¾…ä¸€ä¸ªå®Œæ•´çš„å‘¨æœŸï¼Œç¡®ä¿DOMæ›´æ–°å®Œæ¯•
            await self.wait_for_update(page, self.crawler_settings.get("click_delay", 0.2) * 1000)
    
    async def _collect_all_links_from_sidebar(self, page):
        """
        åœ¨æ‰€æœ‰èœå•éƒ½å±•å¼€åï¼Œä¸€æ¬¡æ€§ä»ä¾§è¾¹æ æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æ–‡æ¡£é“¾æ¥ã€‚
        """
        debug = self.crawler_settings.get("debug_mode", False)
        if debug:
            print("ğŸ”— [Collect] å¼€å§‹ä»ä¾§è¾¹æ æ”¶é›†æ‰€æœ‰é“¾æ¥...")

        sidebar = await page.query_selector("#common-menu-container")
        if not sidebar:
            print("âš ï¸ [Collect] æœªæ‰¾åˆ° #common-menu-container å®¹å™¨ã€‚")
            return []

        # é€‰æ‹©æ‰€æœ‰åŒ…å«hrefçš„<a>æ ‡ç­¾
        all_link_elements = await sidebar.query_selector_all("a[href]")
        if debug:
            print(f"  ğŸ” [Collect] æ‰¾åˆ° {len(all_link_elements)} ä¸ªå¸¦ href çš„ <a> å…ƒç´ ã€‚")

        results = []
        seen_urls = set()

        for link in all_link_elements:
            try:
                href = await link.get_attribute("href")
                text = (await link.text_content() or "").strip()

                # è¿‡æ»¤æ— æ•ˆé“¾æ¥
                if not text or not href or href.strip() == '#':
                    continue
                
                absolute_url = urljoin(self.base_url, href)
                
                # å¿…é¡»æ˜¯é˜¿é‡Œäº‘å¸®åŠ©æ–‡æ¡£çš„é“¾æ¥
                if 'help.aliyun.com' not in absolute_url:
                    continue

                # æ¸…ç†URLç”¨äºå»é‡ï¼ˆç§»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µï¼‰
                clean_url = absolute_url.split('?')[0].split('#')[0]

                if clean_url in seen_urls:
                    continue
                
                seen_urls.add(clean_url)
                # ä¿å­˜åŸå§‹URLï¼Œå› ä¸ºå®ƒå¯èƒ½åŒ…å«å¿…è¦ä¿¡æ¯
                results.append({"url": absolute_url, "title": text})
            except Exception as e:
                if debug:
                    print(f"  âš ï¸ [Collect] å¤„ç†é“¾æ¥ '{href}' æ—¶å‡ºé”™: {e}")

        if debug:
            print(f"âœ… [Collect] æ”¶é›†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆæ–‡æ¡£é“¾æ¥ã€‚")
        return results
    
    async def crawl_document(self, page, url, title):
        """çˆ¬å–å•ä¸ªæ–‡æ¡£"""
        if not self.output_settings['include_content']:
            # åªè¿”å›é“¾æ¥ä¿¡æ¯ï¼Œä¸çˆ¬å–å†…å®¹
            return {
                'url': url,
                'title': title,
                'crawl_time': datetime.now().isoformat()
            }
        
        try:
            await page.goto(url, timeout=self.crawler_settings['wait_timeout'], wait_until='domcontentloaded')
            await asyncio.sleep(0.2)
            
            # å¹¶è¡Œè·å–å†…å®¹
            content_selectors = ['.content-body', '.doc-content', '.article-content', 'main']
            content = ""
            
            for selector in content_selectors:
                content_element = await page.query_selector(selector)
                if content_element:
                    content = await content_element.text_content()
                    if content and len(content.strip()) > 50:
                        break
            
            page_title = await page.title()
            
            return {
                'url': url,
                'title': title or page_title,
                'content': content.strip() if content else "",
                'crawl_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'url': url,
                'title': title,
                'content': "",
                'error': str(e),
                'crawl_time': datetime.now().isoformat()
            }
    
    async def save_product_results(self, product_key, product_info, documents):
        """ä¿å­˜å•ä¸ªäº§å“çš„ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        product_name = product_info['name']
        
        # ä¿å­˜åœ¨é˜¿é‡Œäº‘ä¸“ç”¨ç›®å½•ä¸‹
        # ä¿å­˜é“¾æ¥åˆ—è¡¨
        links_file = self.output_dir / f"aliyun_{product_key}_links_{timestamp}.txt"
        with open(links_file, 'w', encoding='utf-8') as f:
            f.write(f"{product_name}å¸®åŠ©æ–‡æ¡£é“¾æ¥\n")
            f.write("=" * 50 + "\n")
            f.write(f"äº§å“: {product_name}\n")
            f.write(f"æè¿°: {product_info['description']}\n")
            f.write(f"èµ·å§‹URL: {product_info['url']}\n")
            f.write(f"çˆ¬å–æ—¶é—´: {timestamp}\n")
            f.write(f"æ–‡æ¡£æ•°é‡: {len(documents)}\n")
            f.write("=" * 50 + "\n\n")
            
            for i, doc in enumerate(documents, 1):
                f.write(f"{i:3d}. {doc['title']}\n")
                f.write(f"     {doc['url']}\n\n")
        
        # å¦‚æœåŒ…å«å†…å®¹ï¼Œä¿å­˜JSONæ–‡ä»¶
        if self.output_settings['include_content']:
            json_file = self.output_dir / f"aliyun_{product_key}_data_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'product': product_info,
                    'crawl_info': {
                        'timestamp': timestamp,
                        'total_docs': len(documents)
                    },
                    'documents': documents
                }, f, ensure_ascii=False, indent=2)
        
        return links_file, self.output_dir
    
    async def crawl_product(self, product_key, product_info):
        """çˆ¬å–å•ä¸ªäº§å“çš„æ–‡æ¡£"""
        print(f"\nğŸš€ å¼€å§‹çˆ¬å–: {product_info['name']}")
        print(f"ğŸ“ URL: {product_info['url']}")
        print("-" * 60)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.crawler_settings['headless'],
                args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-images']
            )
            
            try:
                context = await browser.new_context()
                page = await context.new_page()
                
                start_time = time.time()
                
                # 1. åŠ è½½é¡µé¢
                print("1ï¸âƒ£ åŠ è½½é¡µé¢...")
                await page.goto(product_info['url'], timeout=self.crawler_settings['wait_timeout'], wait_until='domcontentloaded')
                await self.wait_for_update(page, 500)
                print(f"âœ“ é¡µé¢åŠ è½½å®Œæˆ ({time.time() - start_time:.1f}s)")
                
                # 2. å±•å¼€èœå• (NEW EFFICIENT LOGIC)
                print("2ï¸âƒ£ é«˜æ•ˆå±•å¼€èœå•...")
                if self.crawler_settings.get('debug_mode', False):
                    print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†æ˜¾ç¤ºè¯¦ç»†å±•å¼€è¿‡ç¨‹")
                expand_start = time.time()
                await self._expand_all_menus_dfs(page)
                print(f"âœ“ èœå•å±•å¼€å®Œæˆ ({time.time() - expand_start:.1f}s)")
                
                # 3. æ”¶é›†é“¾æ¥ (NEW EFFICIENT LOGIC)
                print("3ï¸âƒ£ æ”¶é›†æ–‡æ¡£é“¾æ¥...")
                docs_info = await self._collect_all_links_from_sidebar(page)
                print(f"âœ“ æ”¶é›†åˆ° {len(docs_info)} ä¸ªæ–‡æ¡£é“¾æ¥")
                
                if not docs_info:
                    print("âŒ æœªæ‰¾åˆ°æ–‡æ¡£é“¾æ¥")
                    return None
                
                # 4. çˆ¬å–æ–‡æ¡£
                documents = []
                if self.output_settings['include_content']:
                    print(f"4ï¸âƒ£ çˆ¬å– {len(docs_info)} ä¸ªæ–‡æ¡£å†…å®¹...")
                    crawl_start = time.time()
                    
                    for i, doc_info in enumerate(docs_info, 1):
                        if i % 20 == 0:
                            print(f"  è¿›åº¦: {i}/{len(docs_info)} ({i/len(docs_info)*100:.1f}%)")
                        
                        doc_data = await self.crawl_document(page, doc_info['url'], doc_info['title'])
                        documents.append(doc_data)
                        
                        if i < len(docs_info):
                            await asyncio.sleep(self.crawler_settings['crawl_delay'])
                    
                    print(f"âœ“ æ–‡æ¡£çˆ¬å–å®Œæˆ ({time.time() - crawl_start:.1f}s)")
                else:
                    # åªä¿å­˜é“¾æ¥ä¿¡æ¯
                    for doc_info in docs_info:
                        documents.append({
                            'url': doc_info['url'],
                            'title': doc_info['title'],
                            'crawl_time': datetime.now().isoformat()
                        })
                
                # 5. ä¿å­˜ç»“æœ
                print("5ï¸âƒ£ ä¿å­˜ç»“æœ...")
                links_file, output_dir = await self.save_product_results(product_key, product_info, documents)
                
                total_time = time.time() - start_time
                print(f"âœ… {product_info['name']} çˆ¬å–å®Œæˆï¼")
                print(f"ğŸ“„ æ–‡ä»¶ä¿å­˜è‡³: {output_dir}")
                print(f"ğŸ”— é“¾æ¥æ–‡ä»¶: {links_file.name}")
                print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}s")
                
                return {
                    'product_key': product_key,
                    'product_name': product_info['name'],
                    'total_docs': len(documents),
                    'output_dir': str(output_dir),
                    'links_file': str(links_file),
                    'duration': total_time
                }
                
            finally:
                await browser.close()
    
    async def crawl_all_products(self, selected_products=None):
        """çˆ¬å–æ‰€æœ‰äº§å“æˆ–æŒ‡å®šäº§å“çš„æ–‡æ¡£"""
        if selected_products:
            products_to_crawl = {k: v for k, v in self.products.items() if k in selected_products}
        else:
            products_to_crawl = self.products
        
        print(f"ğŸ¯ å‡†å¤‡çˆ¬å– {len(products_to_crawl)} ä¸ªäº§å“çš„å¸®åŠ©æ–‡æ¡£")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
        print("-" * 70)

        results = []
        total_start_time = time.time()
        
        # è¿è¡Œé€‰å®šçš„çˆ¬è™«
        for i, (key, info) in enumerate(products_to_crawl.items(), 1):
            print(f"[{i}/{len(products_to_crawl)}] æ­£åœ¨å¤„ç†: {info['name']} ({key})")
            try:
                res = await self.crawl_product(key, info)
                if res:
                    results.append(res)
            except Exception as e:
                print(f"âŒ çˆ¬å– '{info['name']}' å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        total_elapsed_time = time.time() - total_start_time
        print("\n" + "=" * 70)
        print(f"âœ… æ‰€æœ‰äº§å“çˆ¬å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed_time:.2f}ç§’")
        
        # # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        # await self.generate_summary_report(results, total_elapsed_time)
        
        return results

    # async def generate_summary_report(self, results, total_time):
    #     """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    #     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #     report_file = self.output_dir / f"aliyun_crawl_summary_{timestamp}.txt"
    #     
    #     total_docs = sum(r['doc_count'] for r in results)
    #     
    #     with open(report_file, 'w', encoding='utf-8') as f:
    #         f.write("é˜¿é‡Œäº‘æ–‡æ¡£çˆ¬å–æ€»ç»“æŠ¥å‘Š\n")
    #         f.write("=" * 50 + "\n")
    #         f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}\n")
    #         f.write(f"æ€»è€—æ—¶: {total_time:.2f} ç§’\n")
    #         f.write(f"çˆ¬å–äº§å“æ•°: {len(results)}\n")
    #         f.write(f"æ€»æ–‡æ¡£æ•°: {total_docs}\n")
    #         f.write("=" * 50 + "\n\n")
    #         
    #         for res in results:
    #             f.write(f"äº§å“: {res['product_name']} ({res['product_key']})\n")
    #             f.write(f"  - æ–‡æ¡£æ•°é‡: {res['doc_count']}\n")
    #             f.write(f"  - è€—æ—¶: {res['duration']:.2f} ç§’\n")
    #             f.write(f"  - é“¾æ¥æ–‡ä»¶: {res['links_file']}\n\n")
    #             
    #     print(f"ğŸ“Š æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

async def main():
    """ä¸»å‡½æ•°"""
    crawler = AliyunDocCrawler()
    
    # å¯ä»¥æŒ‡å®šè¦çˆ¬å–çš„äº§å“ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™çˆ¬å–æ‰€æœ‰äº§å“
    # selected_products = ['alb', 'nlb', 'ecs']  # ç¤ºä¾‹ï¼šåªçˆ¬å–è¿™å‡ ä¸ªäº§å“
    selected_products = None  # çˆ¬å–æ‰€æœ‰äº§å“
    
    results = await crawler.crawl_all_products(selected_products)
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªäº§å“")

if __name__ == "__main__":
    asyncio.run(main()) 